import streamlit as st
import os
import csv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage

# =================================================================
# 0. STREAMLIT PAGE CONFIGURATION (MUST BE FIRST)
# =================================================================
st.set_page_config(page_title="Iran-Australia Chatbot", page_icon="ğŸ¤–")

# =================================================================
# 1. CONFIGURATION AND INITIALIZATION (from chatbot.py)
# =================================================================
# IMPORTANT: Hardcoding API keys is generally NOT recommended for production.
# For deployment, consider using Streamlit secrets or environment variables.
os.environ["AVALAI_API_KEY"] = "aa-dhz3cYYm2mJ1LeowMpSDXfrRiy7jQjhUcaNDjN0FWw5Uk9uY"

# Define the base URL for AvalAI
AVALAI_BASE_URL = "https://api.avalai.ir/v1"

# Initialize qa_chain as None. It will be created later if data is available.
qa_chain = None
llm = None
embeddings = None

# Global variables to store user information and chatbot state
# Use Streamlit's session state for persistence across reruns
if 'user_name' not in st.session_state:
    st.session_state.user_name = None
if 'user_phone_number' not in st.session_state:
    st.session_state.user_phone_number = None
if 'user_info_collected' not in st.session_state:
    st.session_state.user_info_collected = False
if 'messages' not in st.session_state:
    st.session_state.messages = []

# =================================================================
# 2. INITIALIZE MODELS AND EMBEDDINGS (from chatbot.py, adapted for Streamlit)
# =================================================================
@st.cache_resource
def initialize_chatbot_components():
    """Initializes LLM, embeddings, and QA chain, caching them."""
    try:
        # Initialize ChatOpenAI for both general use and intent detection
        _llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.7,
            api_key=os.environ["AVALAI_API_KEY"],
            base_url=AVALAI_BASE_URL
        )

        # Initialize OpenAIEmbeddings, pointing to the AvalAI base_url
        _embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            api_key=os.environ["AVALAI_API_KEY"],
            base_url=AVALAI_BASE_URL
        )

        # Load vector store and setup retriever/QA chain
        vector_store_path = "faiss_index" # This must match the path used in embedder.py

        _qa_chain = None
        if os.path.exists(vector_store_path):
            try:
                _vectorstore = FAISS.load_local(vector_store_path, _embeddings, allow_dangerous_deserialization=True)
                _retriever = _vectorstore.as_retriever()
                _qa_chain = RetrievalQA.from_chain_type(
                    llm=_llm,
                    chain_type="stuff",
                    retriever=_retriever
                )
                st.success(f"Vectorstore loaded successfully from {vector_store_path}.")
            except Exception as e:
                st.error(f"âŒ Error loading vectorstore from {vector_store_path}: {e}")
                st.warning("Please ensure embedder.py has been run to create the FAISS index.")
        else:
            st.warning(f"âš ï¸ Warning: Vectorstore not found at {vector_store_path}.")
            st.info("Please run embedder.py first to create the FAISS index. FAQ functionality will be limited.")

        return _llm, _embeddings, _qa_chain

    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ø·ÙˆÙ„ Ø±Ø§Ù‡ Ø§Ù†Ø¯Ø§Ø²ÛŒ Ø±Ø¨Ø§Øª Ú†Øª Ø±Ø® Ø¯Ø§Ø¯: {e}")
        st.info("Ù„Ø·ÙØ§Ù‹ Ú©Ù„ÛŒØ¯ API AvalAIØŒ Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡ØŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª Ùˆ ÙˆØ¶Ø¹ÛŒØª Ø­Ø³Ø§Ø¨ AvalAI Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
        return None, None, None

llm, embeddings, qa_chain = initialize_chatbot_components()

# =================================================================
# 3. INTENT DETECTION FUNCTION (from chatbot.py)
# =================================================================
def detect_intent(user_input: str) -> str:
    """Classifies the intent of the user input."""
    prompt = f"""
You are an intent classifier. Classify the intent of the user input below into one of these:
- greeting
- faq
- complaint
- visitor_info
- chitchat
- unknown
avoid the unknown category as much as possible.
Input: "{user_input}"
Intent:"""
    try:
        response = llm.invoke([HumanMessage(content=prompt)])
        intent_text = response.content.strip()
        # Clean up the intent text to get just the category name
        if ":" in intent_text:
            return intent_text.split(":")[-1].strip()
        else:
            return intent_text
    except Exception as e:
        st.error(f"Error detecting intent: {e}")
        return "unknown"

# =================================================================
# 4. INTENT HANDLERS (from chatbot.py)
# =================================================================
def handle_greeting(query: str):
    """Handles greeting intent, potentially enhanced with info from QA chain, in Persian."""
    base_response = "Ø³Ù„Ø§Ù…ØŒ Ù…Ù† Ø±Ø¨Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ù‡Ø³ØªÙ…ØŒ Ú†Ø¬ÙˆØ±ÛŒ Ù…ÛŒØªÙˆÙ†Ù… Ú©Ù…Ú©ØªÙˆÙ† Ú©Ù†Ù…ØŸ"
    if qa_chain is None:
        return base_response + "\n\n(ØªÙˆØ¬Ù‡: Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ± Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.)"
    try:
        query_for_qa = f"Ù„Ø·ÙØ§ Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ Ùˆ ØªÙˆ Ù‡Ù… Ø³Ù„Ø§Ù… Ùˆ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ Ú©Ù†ØŒ Ø­ÙˆØ§Ø³Øª Ø¨Ø§Ø´Ù‡ Ú©Ù‡ ØªÙˆ Ø±Ø¨Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ù‡Ø³ØªÛŒ.: {query}"
        result = qa_chain.invoke({"query": query_for_qa})
        return "\n" + result['result']
    except Exception as e:
        return "\n\n(ØªÙˆØ¬Ù‡: Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø­ÙˆØ§Ù„Ù¾Ø±Ø³ÛŒ Ø¨Ø§ Ù…Ø´Ú©Ù„ Ù…ÙˆØ§Ø¬Ù‡ Ø´Ø¯.)"
 
def handle_visitor_info(query: str):
    """Handles visitor info intent, potentially enhanced with info from QA chain, in Persian."""
    base_response = "Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯ Ø§ÛŒÙ† Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ù…Ø§ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯! Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø¨Ø²Ø§Ø± ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø±Ø§ÛŒÚ¯Ø§Ù† Ù…Ø§ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÛŒØ²Ø¨Ø§Ù†ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ú©Ø³Ø¨ Ú©Ù†ÛŒØ¯."
    if qa_chain is None:
        return "\n\nØ®Ø·Ø§"
    try:
        query_for_qa = f"Ø³Ø¹ÛŒ Ú©Ù† Ø¯Ø± Ù‚Ø§Ù…Øª ÛŒÚ© Ø±Ø¨Ø§Øª ÙØ±ÙˆØ´ Ù…Ø­ØµÙˆÙ„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ù‚Ø§Ù†Ø¹ Ú©Ù†ÛŒ Ú©Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø²Ø¨Ø§Ù† Ú©Ø§Ø± Ù…ÙÛŒØ¯ÛŒ Ø§Ø³Øª Ùˆ Ø¨Ø§ÛŒØ¯ Ù‡Ø±Ú†Ù‡ Ø²ÙˆØ¯ØªØ± Ø´Ø±ÙˆØ¹ Ú©Ù†Ø¯ Ùˆ Ú©Ø¬Ø§ Ø¨Ù‡ØªØ± Ø§Ø² Ù…ÙˆØ³Ø³Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ØŒ Ø§Ù„Ø¨ØªÙ‡ Ù‚Ø¨Ù„ Ø§Ø² Ù‡Ø± Ú†ÛŒØ² Ø§ÙˆÙ„ Ø¬ÙˆØ§Ø¨ Ø³ÙˆØ§Ù„ Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ Ø±Ùˆ Ø¨Ù‡ Ø·ÙˆØ± Ø¯Ù‚ÛŒÙ‚ Ø¨Ø¯Ù‡ØŒ Ù…Ø«Ù„Ø§ Ø§Ú¯Ù‡ Ù¾Ø±Ø³ÛŒØ¯ Ø¢Ø¯Ø±Ø³ Ú©Ø¬Ø§Ø³ØªØŒ Ø§ÙˆÙ„ Ø¢Ø¯Ø±Ø³ Ø±Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø¨Ú¯ÙˆØŒ Ø¨Ø¹Ø¯ Ù…ØªÙ† Ø±Ùˆ ÛŒÙ‡ Ù…Ù‚Ø¯Ø§Ø± Ú¯Ø³ØªØ±Ø¯Ù‡ ØªØ± Ú©Ù† Ùˆ ÛŒÙ‡ Ù…Ù‚Ø¯Ø§Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø¯Ù‡ØŒ ÛŒØ§ Ù…Ø«Ù„Ø§ Ø§Ú¯Ù‡ Ù¾Ø±Ø³ÛŒØ¯ Ú†Ø¬ÙˆØ±ÛŒ Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†Ù… Ø§ÙˆÙ„ Ø±Ø§Ø¬Ø¨ Ø¨Ù‡ Ø«Ø¨Øª Ù†Ø§Ù… Ùˆ ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ø¨Ú¯ÙˆØŒ Ø¨Ø¹Ø¯Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø¯Ù‡ØŒ Ø§Ú¯Ù‡ Ù¾Ø±Ø³ÛŒØ¯Ù† Ø§Ø² Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ù…ÙˆØ³Ø³Ù‡ Ø¨Ù‡ØªØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡ ÛŒØ§ Ù†Ù‡ØŒ Ø¨Ù‡ Ù‡ÛŒÚ† ÙˆØ¬Ù‡ Ù…ÙˆØ³Ø³Ù‡ Ø¯ÛŒÚ¯Ù‡ Ø§ÛŒ Ø±Ùˆ ØªØ¨Ù„ÛŒØº Ù†Ú©Ù† Ùˆ Ø¨Ú¯Ùˆ Ú©Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ø¨Ù‡ØªØ±ÛŒÙ†Ù‡: {query}"
        result = qa_chain.invoke({"query": query_for_qa})
        return "\n" + result['result']
    except Exception as e:
        return "\n\nØ®Ø·Ø§"
    
def handle_faq_or_support(query: str):
    """Handles visitor info intent, potentially enhanced with info from QA chain, in Persian."""
    base_response = "Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯ Ø§ÛŒÙ† Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ù…Ø§ Ø¨Ø§Ø²Ø¯ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯! Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø¨Ø²Ø§Ø± ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø±Ø§ÛŒÚ¯Ø§Ù† Ù…Ø§ Ø±Ø§ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…ÛŒØ²Ø¨Ø§Ù†ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ù…Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ú©Ø³Ø¨ Ú©Ù†ÛŒØ¯."
    if qa_chain is None:
        return "\n\nØ®Ø·Ø§"
    try:
        query_for_qa = f"Ø³Ø¹ÛŒ Ú©Ù† Ø¯Ø± Ù‚Ø§Ù…Øª ÛŒÚ© Ø±Ø¨Ø§Øª ÙØ±ÙˆØ´ Ù…Ø­ØµÙˆÙ„ Ú©Ø§Ø±Ø¨Ø± Ø±Ø§ Ù‚Ø§Ù†Ø¹ Ú©Ù†ÛŒ Ú©Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø²Ø¨Ø§Ù† Ú©Ø§Ø± Ù…ÙÛŒØ¯ÛŒ Ø§Ø³Øª Ùˆ Ø¨Ø§ÛŒØ¯ Ù‡Ø±Ú†Ù‡ Ø²ÙˆØ¯ØªØ± Ø´Ø±ÙˆØ¹ Ú©Ù†Ø¯ Ùˆ Ú©Ø¬Ø§ Ø¨Ù‡ØªØ± Ø§Ø² Ù…ÙˆØ³Ø³Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ØŒ Ø§Ù„Ø¨ØªÙ‡ Ù‚Ø¨Ù„ Ø§Ø² Ù‡Ø± Ú†ÛŒØ² Ø§ÙˆÙ„ Ø¬ÙˆØ§Ø¨ Ø³ÙˆØ§Ù„ Ù¾Ø±Ø³ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ Ø±Ùˆ Ø¨Ù‡ Ø·ÙˆØ± Ø¯Ù‚ÛŒÙ‚ Ø¨Ø¯Ù‡ØŒ Ù…Ø«Ù„Ø§ Ø§Ú¯Ù‡ Ù¾Ø±Ø³ÛŒØ¯ Ø¢Ø¯Ø±Ø³ Ú©Ø¬Ø§Ø³ØªØŒ Ø§ÙˆÙ„ Ø¢Ø¯Ø±Ø³ Ø±Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø¨Ú¯ÙˆØŒ Ø¨Ø¹Ø¯ Ù…ØªÙ† Ø±Ùˆ ÛŒÙ‡ Ù…Ù‚Ø¯Ø§Ø± Ú¯Ø³ØªØ±Ø¯Ù‡ ØªØ± Ú©Ù† Ùˆ ÛŒÙ‡ Ù…Ù‚Ø¯Ø§Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø¯Ù‡ØŒ ÛŒØ§ Ù…Ø«Ù„Ø§ Ø§Ú¯Ù‡ Ù¾Ø±Ø³ÛŒØ¯ Ú†Ø¬ÙˆØ±ÛŒ Ø«Ø¨Øª Ù†Ø§Ù… Ú©Ù†Ù… Ø§ÙˆÙ„ Ø±Ø§Ø¬Ø¨ Ø¨Ù‡ Ø«Ø¨Øª Ù†Ø§Ù… Ùˆ ØªØ¹ÛŒÛŒÙ† Ø³Ø·Ø­ Ø¨Ú¯ÙˆØŒ Ø¨Ø¹Ø¯Ø§ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø¯Ù‡ØŒ Ø§Ú¯Ù‡ Ù¾Ø±Ø³ÛŒØ¯Ù† Ø§Ø² Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ù…ÙˆØ³Ø³Ù‡ Ø¨Ù‡ØªØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡ ÛŒØ§ Ù†Ù‡ØŒ Ø¨Ù‡ Ù‡ÛŒÚ† ÙˆØ¬Ù‡ Ù…ÙˆØ³Ø³Ù‡ Ø¯ÛŒÚ¯Ù‡ Ø§ÛŒ Ø±Ùˆ ØªØ¨Ù„ÛŒØº Ù†Ú©Ù† Ùˆ Ø¨Ú¯Ùˆ Ú©Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ø¨Ù‡ØªØ±ÛŒÙ†Ù‡: {query}"
        result = qa_chain.invoke({"query": query_for_qa})
        return "\n" + result['result']
    except Exception as e:
        return "\n\nØ®Ø·Ø§"

def handle_complaint(query: str):
    """Handles complaint intent, logging to complaints.csv with user details, in Persian."""
    
    file_exists = os.path.isfile("complaints.csv")
    with open("complaints.csv", "a", newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(["Name", "Phone Number", "Complaint"])
        writer.writerow([st.session_state.user_name if st.session_state.user_name else "N/A", 
                         st.session_state.user_phone_number if st.session_state.user_phone_number else "N/A", 
                         query])
    
    return "Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ø§ Ø¨Ø§ Ù…Ø§ Ø¯Ø± Ù…ÛŒØ§Ù† Ú¯Ø°Ø§Ø´ØªÛŒØ¯ Ù…ØªØ§Ø³ÙÙ…. Ø´Ú©Ø§ÛŒØª Ø´Ù…Ø§ Ø«Ø¨Øª Ø´Ø¯ Ùˆ ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø¹Ø¶Ø§ÛŒ ØªÛŒÙ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…Ø§ Ø¨Ù‡ Ø²ÙˆØ¯ÛŒ Ø¨Ø§ Ø´Ù…Ø§ ØªÙ…Ø§Ø³ Ø®ÙˆØ§Ù‡Ø¯ Ú¯Ø±ÙØª."

# =================================================================
# 5. MAIN CHATBOT PIPELINE (from chatbot.py, adapted for Streamlit)
# =================================================================
def chatbot_response(user_input: str):
    """The main function that routes user input to the correct handler."""
    
    if not st.session_state.user_info_collected:
        if st.session_state.user_name is None:
            st.session_state.user_name = user_input.strip()
            return "Ù…ØªØ´Ú©Ø±Ù…ØŒ Ù„Ø·ÙØ§ Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:"
        elif st.session_state.user_phone_number is None:
            st.session_state.user_phone_number = user_input.strip()
            st.session_state.user_info_collected = True
            return f"Ø³Ù„Ø§Ù… {st.session_state.user_name}! Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† Ø´Ù…Ø§ ({st.session_state.user_phone_number}) Ø«Ø¨Øª Ø´Ø¯. Ø­Ø§Ù„Ø§ Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ú©Ù†Ù…ØŸ"
        else:
            return "Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ù…Ø§ Ø±Ø§ Ø¯Ø§Ø±Ù…. Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ú©Ù†Ù…ØŸ"

    intent = detect_intent(user_input)
    st.session_state.messages.append({"role": "assistant", "content": f"(Ù‚ØµØ¯ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡: {intent})"}) # For debugging

    if intent == "greeting":
        return handle_greeting(user_input)
    elif intent == "visitor_info":
        return handle_visitor_info(user_input)
    elif intent == "faq":
        return handle_faq_or_support(user_input)
    elif intent == "complaint":
        return handle_complaint(user_input)
    elif intent == "chitchat":
        if llm is None:
            return "Ù…ØªØ§Ø³ÙÙ…ØŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù… Ø²ÛŒØ±Ø§ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù† Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."
        prompt = f"""
Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ Ø§ÛŒÙ† Ù¾ÛŒØ§Ù… Ø¨Ù‡ ØµÙˆØ±Øª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø¬Ø§Ù…Ø¹ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:
Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±: "{user_input}"
Ù¾Ø§Ø³Ø®:"""
        try:
            response = llm.invoke([HumanMessage(content=prompt)])
            return response.content.strip()
        except Exception as e:
            return f"Ù…ØªØ§Ø³ÙÙ…ØŒ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù…. Ø®Ø·Ø§ÛŒ Ø±Ø® Ø¯Ø§Ø¯Ù‡: {e}"
    else: # Handles 'unknown'
        return "Ù…Ù† Ù‡Ù†ÙˆØ² Ù…Ø·Ù…Ø¦Ù† Ù†ÛŒØ³ØªÙ… Ú†Ú¯ÙˆÙ†Ù‡ Ø¨Ù‡ Ø´Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† Ù…ÙˆØ±Ø¯ Ú©Ù…Ú© Ú©Ù†Ù…. Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ø³ÙˆØ§Ù„ÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø®Ø¯Ù…Ø§Øª ÛŒØ§ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯."

# =================================================================
# 6. STREAMLIT UI
# =================================================================
st.title("ğŸ¤– Iran-Australia Chatbot")
st.markdown("Ø¨Ù‡ Ø±Ø¨Ø§Øª Ú†Øª Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø§ÛŒØ±Ø§Ù† Ø§Ø³ØªØ±Ø§Ù„ÛŒØ§ Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯. Ù…Ù† Ø§ÛŒÙ†Ø¬Ø§ Ù‡Ø³ØªÙ… ØªØ§ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ù… Ùˆ Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ú©Ù†Ù….")

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Initial prompt for user information if not collected
if not st.session_state.user_info_collected:
    if st.session_state.user_name is None:
        initial_prompt = "Ø³Ù„Ø§Ù…! Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ØŒ Ù„Ø·ÙØ§Ù‹ Ù†Ø§Ù… Ú©Ø§Ù…Ù„ Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:"
    elif st.session_state.user_phone_number is None:
        initial_prompt = "Ù…ØªØ´Ú©Ø±Ù…ØŒ Ù„Ø·ÙØ§ Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯:"
    else:
        initial_prompt = f"Ø³Ù„Ø§Ù… {st.session_state.user_name}! Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† Ø´Ù…Ø§ ({st.session_state.user_phone_number}) Ø«Ø¨Øª Ø´Ø¯. Ø­Ø§Ù„Ø§ Ú†Ú¯ÙˆÙ†Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ú©Ù†Ù…ØŸ"
    
    # Display the initial prompt as a bot message
    if not st.session_state.messages or st.session_state.messages[-1]["content"] != initial_prompt:
        st.session_state.messages.append({"role": "assistant", "content": initial_prompt})
        with st.chat_message("assistant"):
            st.markdown(initial_prompt)

# Accept user input
if prompt := st.chat_input("Ù¾ÛŒØ§Ù… Ø®ÙˆØ¯ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Get chatbot response
    with st.chat_message("assistant"):
        with st.spinner("Ø¯Ø± Ø­Ø§Ù„ ÙÚ©Ø± Ú©Ø±Ø¯Ù†..."):
            response = chatbot_response(prompt)
            st.markdown(response)
    # Add assistant response to chat history
    st.session_state.messages.append({"role": "assistant", "content": response})